{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Neuromatch Academy - Deep Learning: Week 1, Day 4\n",
    "# Optimization\n",
    "\n",
    "__Content creators:__ Ioannis Mitliagkas, Jose Gallego-Posada\n",
    "\n",
    "__Content reviewers:__ Name Surname, Name Surname \n",
    "\n",
    "__Content editors:__ Name Surname, Name Surname\n",
    "\n",
    "__Production editors:__ Arush Tagade, Spiros Chavlis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#Tutorial Objectives\n",
    "\n",
    "Objectives:\n",
    "*   Necessity and Importance of Optimization\n",
    "*   Introduction to commonly used optimization techniques\n",
    "*   Optimization in non-convex loss landscapes \n",
    "*   Learn about Adaptive Hyperparameter Tuning \n",
    "*   Ethical concerns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import io\n",
    "from urllib.request import urlopen\n",
    "import copy\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision\n",
    "\n",
    "torch.manual_seed(31051832)\n",
    "np.random.seed(31051832)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Figure settings\n",
    "import ipywidgets as widgets       # interactive display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
    "\n",
    "plt.rc('axes', unicode_minus=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "8b5dd914-9042-4f00-9f8d-35998c69af73"
   },
   "outputs": [],
   "source": [
    "# TEMPLATE\n",
    "\n",
    "#@title Video 1: Introduction\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"zm9oekdkJbQ\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Case study: successfully training an MLP for image classification\n",
    "\n",
    "Many of the core ideas (and tricks) in modern optimization for deep learning can be illustrated in the simple setting of training an MLP to solve an image classification task. In this tutorial we will guide you through the key challenges that arise when optimizing high-dimensional, non-convex problems, as well as some commonly used solutions.\n",
    "\n",
    "**Disclaimer:** Some of the functions you will code in this tutorial are already implemented in Pytorch and many other libraries. For pedagogical reasons, we decided to bring these simple coding tasks into the spotlight and place a relatively higher emphasis in your understanding of the algorithms, rather than the use of a specific library. \n",
    "\n",
    "In 'day-to-day' research projects you will likely to rely on the community-vetted, optimized libraries rather than the 'manual implementations' you will write today. In Section 8 you will have a chance to 'put it all together' and use the full power of Pytorch to tune the parameters of an MLP to classify handwritten digits.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "57d4f0e2-fb1e-481d-cf01-6211e46481f5"
   },
   "outputs": [],
   "source": [
    "#@title Video 2: Case Study - MLP Classification\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"pJc2ENhYbqA\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will use the MNIST dataset of handwritten digits. We load the data via the Pytorch `datasets` module, as you learned in Day 1.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data(change_tensors=False):\n",
    "    \"\"\"Load training and test examples for the MNIST digits dataset\n",
    "\n",
    "    Returns:\n",
    "        train_data (tensor): training input tensor of size (train_size x 784)\n",
    "        train_target (tensor): training 0-9 integer label tensor of size (train_size)\n",
    "        test_data (tensor): test input tensor of size (70k-train_size x 784)\n",
    "        test_target (tensor): training 0-9 integer label tensor of size (70k-train_size)\n",
    "\n",
    "    \"\"\"\n",
    "    # Load train and test sets\n",
    "    train_set = datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "    test_set = datasets.MNIST(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "    # Original data is in range [0, 255]. We normalize the data wrt its mean and std_dev.\n",
    "    # # Note that we only used *training set* information to compute mean and std\n",
    "    mean = train_set.data.float().mean()\n",
    "    std = train_set.data.float().std()\n",
    "\n",
    "    if change_tensors:\n",
    "        # Apply normalization directly to the tensors containing the dataset\n",
    "        train_set.data = (train_set.data.float() - mean) / std\n",
    "        test_set.data = (test_set.data.float() - mean) / std\n",
    "    else:\n",
    "\n",
    "        tform = torchvision.transforms.Compose([\n",
    "                            torchvision.transforms.ToTensor(),\n",
    "                            torchvision.transforms.Normalize(mean=[mean/255.],\n",
    "                                                             std=[std/255.])\n",
    "                                    ])\n",
    "        train_set = datasets.MNIST(root='./data', train=True, download=True, transform=tform)\n",
    "        test_set = datasets.MNIST(root='./data', train=False, download=True, transform=tform)\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "train_set, test_set = load_mnist_data(change_tensors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are just getting started, we will concentrate on a small subset of only 500 examples out of the 60.000 data points contained in the whole training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a random subset of 500 indices\n",
    "subset_index = np.random.choice(len(train_set.data), 500)\n",
    "\n",
    "# We will use these symbols to represent the training data and labels, to stay\n",
    "# as close to the mathematical expressions as possible.\n",
    "X, y = train_set.data[subset_index, :], train_set.targets[subset_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "outputId": "cf9a717b-ad2c-4523-ce7a-8ce6e65cc1d5"
   },
   "outputs": [],
   "source": [
    "#@markdown Run this cell to visualize the content of three examples in our\n",
    "#@markdown training set. Note how the pre-processing we applied to the data changes\n",
    "#@markdown the range of pixel values after normalization.\n",
    "\n",
    "num_figures = 3\n",
    "fig, axs = plt.subplots(1, num_figures, figsize=(5*num_figures, 5))\n",
    "\n",
    "for sample_id, ax in enumerate(axs):\n",
    "    # Plot the pixel values for each image\n",
    "    ax.matshow(X[sample_id, :], cmap='gray_r')\n",
    "    # 'Write' the pixel value in the corresponding location\n",
    "    for (i, j), z in np.ndenumerate(X[sample_id, :]):\n",
    "        text = '{:.1f}'.format(z)\n",
    "        ax.text(j, i, text, ha='center', va='center', fontsize=6, c='steelblue')\n",
    "\n",
    "    ax.set_title('Label: ' + str(y[sample_id].item()))\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "As you will see in Week 2 Day 1, there are specific model architectures that are better suited to image-like data, such as Convolutional Neural Networks (CNNs). For simplicity, in this tutorial we will focus exclusively on Multi-Layer Perceptron (MLP) models as they allow us to highlight many important optimization challenges shared with more advanced neural network designs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\" This class implements MLPs in Pytorch of an arbitrary number of hidden\n",
    "    layers of potentially different sizes. Since we concentrate on classification\n",
    "    tasks in this tutorial, we have a log_softmax layer at prediction time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim=784, out_dim=10, hidden_dims=[], use_bias=True):\n",
    "        \"\"\"Constructs a MultiLayerPerceptron\n",
    "\n",
    "        Args:\n",
    "            in_dim (int): dimensionality of input data\n",
    "            out_dim (int): number of classes\n",
    "            hidden_dims (list): contains the dimensions of the hidden layers, an empty\n",
    "                list corresponds to a linear model (in_dim, out_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        # If we have no hidden layer, just initialize a linear model (e.g. in logistic regression)\n",
    "        if len(hidden_dims) == 0:\n",
    "            layers = [nn.Linear(in_dim, out_dim, bias=use_bias)]\n",
    "        else:\n",
    "            # 'Actual' MLP with dimensions in_dim - num_hidden_layers*[hidden_dim] - out_dim\n",
    "            layers = [nn.Linear(in_dim, hidden_dims[0], bias=use_bias), nn.ReLU()]\n",
    "\n",
    "            # Loop until before the last layer\n",
    "            for hl_id in range(len(hidden_dims)-1):\n",
    "                layers += [nn.Linear(hidden_dims[hl_id], hidden_dims[hl_id+1], bias=use_bias), nn.ReLU()]\n",
    "\n",
    "            # Add final layer to the number of classes\n",
    "            layers.append(nn.Linear(hidden_dims[-1], out_dim, bias=use_bias))\n",
    "\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the images into 'vectors'\n",
    "        x = x.view(-1, self.in_dim)\n",
    "        x = self.main(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models constitute a very special kind of MLPs: they are equivalent to an MLP with *zero* hidden layers.\n",
    "\n",
    "$$f(x) = \\text{softmax}(W x + b)$$\n",
    "\n",
    "Here $x \\in \\mathbb{R}^{784}$, $W \\in \\mathbb{R}^{10 \\times 784}$ and $b \\in \\mathbb{R}^{10}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty hidden_dims means we take a model with zero hidden layers.\n",
    "model = MLP(in_dim=784, out_dim=10, hidden_dims=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "While we care about the accuracy of the model, the 'discrete' nature of the 0-1 loss makes it challenging to optimize. In order to learn good parameters for this model, we will use the cross entropy loss (negative log-likelihood), which you saw in last lecture, as a surrogate objective to be minimized. \n",
    "\n",
    "This particular choice of model and optimization objective leads to a *convex* optimization problem with respect to the parameters $W$ and $b$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@markdown In last lecture, you saw that inspecting the weights of a model can provide\n",
    "#@markdown insights on what 'concepts' the model has learned. Here we show the weights\n",
    "#@markdown of a partially trained model. The weights corresponding to each class\n",
    "#@markdown 'learn' to _fire_ when an input of the class is detected.\n",
    "\n",
    "#@markdown Run _this cell_ to train the model. If you are curious about how the training\n",
    "#@markdown takes place, double-click this cell to find out. At the end of this tutorial\n",
    "#@markdown you will have the opportunity to train a more complex model on your own.\n",
    "\n",
    "cell_verbose = False\n",
    "partial_trained_model = MLP(in_dim=784, out_dim=10, hidden_dims=[])\n",
    "\n",
    "if cell_verbose:\n",
    "    print('Init loss', loss_fn(partial_trained_model(X), y).item()) # This matches around np.log(10 = # of classes)\n",
    "\n",
    "optimizer = optim.Adam(partial_trained_model.parameters(), lr=7e-4)\n",
    "for _ in range(200):\n",
    "    loss = loss_fn(partial_trained_model(X), y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "if cell_verbose:\n",
    "    print('End loss', loss_fn(partial_trained_model(X), y).item()) # This should be less than 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "outputId": "087fdb9b-fb7b-488a-dd5c-0818f02b3435"
   },
   "outputs": [],
   "source": [
    "# Show class filters of a trained model\n",
    "W = partial_trained_model.main[0].weight.data.numpy()\n",
    "\n",
    "with plt.xkcd():\n",
    "    fig, axs = plt.subplots(1, 10, figsize=(15,4))\n",
    "    for class_id in range(10):\n",
    "        axs[class_id].imshow(W[class_id, :].reshape(28, 28), cmap='gray_r')\n",
    "        axs[class_id].axis('off')\n",
    "        axs[class_id].set_title('Class ' + str(class_id) )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. High dimensional search\n",
    "\n",
    "We now have a model with its corresponding trainable parameters as well as an objective to optimize. Where do we go to next? How do we find a 'good' configuration of parameters?\n",
    "\n",
    "One idea is to choose a random direction and move only if the objective is reduced. However, this is inefficient in high dimensions and you will see how gradient descent (with a suitable step-size) can guarantee consistent improvement in terms of the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "578db176-2f49-409f-a01d-0c69b4c3f461"
   },
   "outputs": [],
   "source": [
    "#@title Video 3: Optimization of an Objective Function\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"aSJTRdjRvvw\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement gradient descent\n",
    "\n",
    "In this exercise you will use Pytorch automatic differentiation capabilities to compute the gradient of the loss with respect to the parameters of the model. You will then use these gradients to implement the update performed by the gradient descent method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grad(params):\n",
    "    \"\"\"Clear up gradients as Pytorch automatically accumulates gradients from\n",
    "    successive backward calls\n",
    "    \"\"\"\n",
    "    for par in params:\n",
    "        if not(par.grad is None):\n",
    "            par.grad.data.zero_()\n",
    "\n",
    "def random_update(model, noise_scale=0.1, normalized=False):\n",
    "    \"\"\" Performs a random update on the parameters of the model\n",
    "    \"\"\"\n",
    "    for par in model.parameters():\n",
    "        noise = torch.randn_like(par)\n",
    "        if normalized:\n",
    "            noise /= torch.norm(noise)\n",
    "        par.data +=  noise_scale * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_update(loss, params, lr=1e-1):\n",
    "    \"\"\"Perform a gradient descent update on a given loss over a collection of parameters\n",
    "\n",
    "    Args:\n",
    "        loss (tensor): A scalar tensor containing the loss whose gradient will be computed\n",
    "        params (iterable): Collection of parameters with respect to which we compute gradients\n",
    "        lr (float): Scalar specifying the learning rate or step-size for the update\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Clear up gradients as Pytorch automatically accumulates gradients from\n",
    "    # successive backward calls\n",
    "    zero_grad(params)\n",
    "\n",
    "    # Compute gradients on given objective\n",
    "    loss.backward()\n",
    "\n",
    "    for par in params:\n",
    "        #################################################\n",
    "        ## TODO for students: update the value of the parameter ##\n",
    "\n",
    "        # Here we work with the 'data' attribute of the parameter rather than the\n",
    "        # parameter itself. Use the random_update function above to inspire your solution.\n",
    "        par.data = ...\n",
    "\n",
    "        raise NotImplementedError(\"Student exercise: implement gradient update\")\n",
    "        #################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D4_Optimization/solutions/W1D4_tutorial_Solution_971af708.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "outputId": "fb614c92-436b-46e0-ac1f-742391bf66d2"
   },
   "outputs": [],
   "source": [
    "#@markdown These plots compare the effectiveness of updating random\n",
    "#@markdown directions for the problem of optimizing the parameters\n",
    "#@markdown of a high-dimensional linear model. We contrast the behavior at\n",
    "#@markdown initialization and during an intermediate stage of training by showing\n",
    "#@markdown the histograms of change in loss over 100 different random directions\n",
    "#@markdown vs the changed in loss induced by the gradient descent update -- negative is better.\n",
    "\n",
    "with plt.xkcd():\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10,4))\n",
    "\n",
    "    for id, (model_name, my_model) in enumerate([('Initialization', model),\n",
    "                                ('Partially trained', partial_trained_model)]):\n",
    "\n",
    "        # Compue the loss we will be comparing to\n",
    "        base_loss = loss_fn(my_model(X), y)\n",
    "\n",
    "        # Compute the improvement via gradient descent\n",
    "        dummy_model = copy.deepcopy(my_model)\n",
    "        loss1 = loss_fn(dummy_model(X), y)\n",
    "        gradient_update(loss1, list(dummy_model.parameters()), lr=1e-2)\n",
    "        gd_delta = loss_fn(dummy_model(X), y) - base_loss\n",
    "\n",
    "        deltas = []\n",
    "        for trial_id in range(100):\n",
    "            # Compute the improvement obtained with a random direction\n",
    "            dummy_model = copy.deepcopy(my_model)\n",
    "            random_update(dummy_model, noise_scale=1e-2)\n",
    "            deltas.append((loss_fn(dummy_model(X), y) - base_loss).item())\n",
    "\n",
    "        # Plot histogram for random direction and vertical line for gradient descent\n",
    "        axs[id].hist(deltas, label='Random Directions', bins=20)\n",
    "        axs[id].set_title(model_name)\n",
    "        axs[id].set_xlabel('Change in loss')\n",
    "        axs[id].axvline(0, c='green', alpha=0.5)\n",
    "        axs[id].axvline(gd_delta.item(), linestyle='--', c='red', alpha=1, label='Gradient Descent')\n",
    "\n",
    "\n",
    "handles, labels = axs[id].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center',\n",
    "           bbox_to_anchor=(0.5, 1.05), fancybox=False, shadow=False, ncol=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks:** Note how gradient descent (with a properly tuned step size) guarantees an improvement in the objective function, unlike the random udpate method, which very rarely finds a direction that improves the objective function. The magnitue of the improvement at initialization is much bigger than the change in loss obtained at a later stage in training. This is expected, as we have gotten closer to the optimum parameter configuration.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Poor conditioning\n",
    "\n",
    "Already in this 'simple' logistic regression problem, the issue of bad conditioning is haunting us. Not all parameters are created equal and the sensitivity of the network to changes on the parameters will have a big impact in the dynamics of the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "d50c487d-2bf0-4b86-fc2f-80dcc40dde9a"
   },
   "outputs": [],
   "source": [
    "#@title Video 4: Momentum\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"3ES5O58Y_2M\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@markdown We illustrate this issue in a 2-dimensional setting. We freeze all but\n",
    "#@markdown two parameters of the network: one of them is an element of the weight matrix (filter)\n",
    "#@markdown for class 0, while the other is the bias for class 7. This results in\n",
    "#@markdown an optimization with two decision variables.\n",
    "\n",
    "#@markdown How much difference is there in the behavior of these two parameters\n",
    "#@markdown under gradient descent? What is the effect of momentum in bridging that gap?\n",
    "\n",
    "#@markdown Run _this cell_ to setup some helper functions.\n",
    "\n",
    "def loss_2d(model, u, v, mask_idx=(0, 378), bias_id=7):\n",
    "    \"\"\"Defines a 2-dim function by freezing all but two parameters of a linear\n",
    "    model.\n",
    "\n",
    "    Args:\n",
    "        model (torch module): a pytorch 0-hidden layer (linear) model\n",
    "        u (scalar): first free parameter\n",
    "        v (scalar): second free parameter\n",
    "        mask_idx (tuple): selects parameter in weight matrix replaced by u\n",
    "        bias_idx (int): selects parameter in bias vector replaced by v\n",
    "\n",
    "    Returns:\n",
    "        scalar: loss of the 'new' model over inputs X, y (defined externally)\n",
    "    \"\"\"\n",
    "\n",
    "    # We zero out the element of the weight tensor that will be\n",
    "    # replaced by u\n",
    "    mask = torch.ones_like(model.main[0].weight)\n",
    "    mask[mask_idx[0], mask_idx[1]] = 0.\n",
    "    masked_weights = model.main[0].weight * mask\n",
    "\n",
    "    # u is replacing an element of the weight matrix\n",
    "    masked_weights[mask_idx[0], mask_idx[1]] = u\n",
    "\n",
    "    res = X.reshape(-1, 784) @ masked_weights.T + model.main[0].bias\n",
    "\n",
    "    # v is replacing a bias for class 7\n",
    "    res[:, 7] += v - model.main[0].bias[7]\n",
    "    res =  F.log_softmax(res, dim=1)\n",
    "\n",
    "    return loss_fn(res, y)\n",
    "\n",
    "def plot_surface(U, V, Z, fig):\n",
    "    \"\"\" Plot a 3D loss surface given meshed inputs U, V and values Z\n",
    "    \"\"\"\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    ax.view_init(45, -130)\n",
    "\n",
    "    surf = ax.plot_surface(U, V, Z, cmap=plt.cm.coolwarm,\n",
    "                        linewidth=0, antialiased=True, alpha=0.5)\n",
    "\n",
    "    # Select certain level contours to plot\n",
    "    # levels = Z.min() * np.array([1.005, 1.1, 1.3, 1.5, 2.])\n",
    "    # plt.contour(U, V, Z)# levels=levels, alpha=0.5)\n",
    "\n",
    "    ax.set_xlabel('Weight')\n",
    "    ax.set_ylabel('Bias')\n",
    "    ax.set_zlabel('Loss', rotation=90)\n",
    "\n",
    "    return ax\n",
    "\n",
    "def plot_param_distance(best_u, best_v, trajs, fig, styles, labels, use_log=False):\n",
    "    \"\"\" Plot the distance to each of the two parameters for a collection of 'trajectories'\n",
    "    \"\"\"\n",
    "    with plt.xkcd():\n",
    "\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "        for traj, style, label in zip(trajs, styles, labels):\n",
    "\n",
    "            d0 = [np.abs(_[0] - best_u) for _ in traj]\n",
    "            d1 = [np.abs(_[1] - best_v) for _ in traj]\n",
    "\n",
    "            if use_log:\n",
    "                d0 = np.log(d0)\n",
    "                d1 = np.log(d1)\n",
    "\n",
    "            ax.plot(range(len(traj)), d0, style, label='weight - ' + label)\n",
    "            ax.plot(range(len(traj)), d1, style, label='bias - ' + label)\n",
    "\n",
    "        ax.set_xlabel('Iteration')\n",
    "        if use_log:\n",
    "            ax.set_ylabel('Log Abs distance to optimum')\n",
    "        else:\n",
    "            ax.set_ylabel('Abs distance to optimum')\n",
    "        ax.legend(loc='right', bbox_to_anchor=(1.5, 0.5),\n",
    "                  fancybox=False, shadow=False, ncol=1)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def run_optimizer(inits, eval_fn, update_fn, max_steps=500, optim_kwargs={'lr':1e-2},\n",
    "                  log_traj=True):\n",
    "\n",
    "    \"\"\"Runs an optimizer on a given objective and logs parameter trajectory\n",
    "\n",
    "    Args:\n",
    "        inits list(scalar): initialization of parameters\n",
    "        eval_fn (callable): function computing the objective to be minimized\n",
    "        update_fn (callable): function executing parameter update\n",
    "        max_steps (int): number of iterations to run\n",
    "        optim_kwargs (dict): customize optimizer hyperparameters\n",
    "\n",
    "    Returns:\n",
    "        list[list]: trajectory information [*params, loss] for each optimization step\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize parameters and optimizer\n",
    "    params = [nn.Parameter(torch.tensor(_)) for _ in inits]\n",
    "\n",
    "    # Methods like momentum and rmsprop keep and auxiliary vector of parameters\n",
    "    aux_tensors = [torch.zeros_like(_) for _ in params]\n",
    "\n",
    "    if log_traj:\n",
    "        traj = np.zeros((max_steps, len(params)+1))\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "\n",
    "        # Evaluate loss\n",
    "        loss = eval_fn(*params)\n",
    "\n",
    "        # Store 'trajectory' information\n",
    "        if log_traj:\n",
    "            traj[_, :] = [_.item() for _ in params] + [loss.item()]\n",
    "\n",
    "        # Perform update\n",
    "        if update_fn == gradient_update:\n",
    "            gradient_update(loss, params, **optim_kwargs)\n",
    "        else:\n",
    "            update_fn(loss, params, aux_tensors, **optim_kwargs)\n",
    "\n",
    "    if log_traj:\n",
    "        return traj\n",
    "\n",
    "\n",
    "L = 4.\n",
    "xs = np.linspace(-L, L, 30)\n",
    "ys = np.linspace(-L, L, 30)\n",
    "U, V = np.meshgrid(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement momentum\n",
    "\n",
    "In this exercise you will implement the momentum update given by:\n",
    "$$w_{t+1} = w_t - \\eta \\nabla J(w_t) + \\beta (w_t - w_{t-1})$$\n",
    "\n",
    "It is convenient to re-express this update rule in terms of a recursion. For that, we define the quantity:\n",
    "$$v_{t-1} := w_{t} - w_{t-1},$$\n",
    "which leads to the two-step update rule:\n",
    "$$ v_t = - \\eta \\nabla J(w_t) + \\beta (\\underbrace{w_t - w_{t-1}}_{v_{t-1}})$$\n",
    "$$ w_{t+1} \\leftarrow w_t + v_{t}$$\n",
    "\n",
    "Pay attention to the positive sign of the update in the last equation, given the definition of $v_t$, above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_update(loss, params, grad_vel, lr=1e-1, beta=0.8):\n",
    "    \"\"\"Perform a momentum update over a collection of parameters given a loss and 'velocities'\n",
    "\n",
    "    Args:\n",
    "        loss (tensor): A scalar tensor containing the loss whose gradient will be computed\n",
    "        params (iterable): Collection of parameters with respect to which we compute gradients\n",
    "        grad_vel (iterable): Collection containing the 'velocity' v_t for each parameter\n",
    "        lr (float): Scalar specifying the learning rate or step-size for the update\n",
    "        beta (float): Scalar 'momentum' parameter\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Clear up gradients as Pytorch automatically accumulates gradients from\n",
    "    # successive backward calls\n",
    "    zero_grad(params)\n",
    "\n",
    "    # Compute gradients on given objective\n",
    "    loss.backward()\n",
    "\n",
    "    for (par, vel) in zip(params, grad_vel):\n",
    "\n",
    "        #################################################\n",
    "        ## TODO for students: update the value of the parameter ##\n",
    "\n",
    "        # Update 'velocity'\n",
    "        vel.data = ...\n",
    "\n",
    "        # Update parameters\n",
    "        par.data = ...\n",
    "\n",
    "        raise NotImplementedError(\"Student exercise: implement momentum update\")\n",
    "        #################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D4_Optimization/solutions/W1D4_tutorial_Solution_aa40d1b2.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum vs GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694,
     "referenced_widgets": [
      "91dc34912a094eb18e89139c5f1611a2",
      "4562d5e534814bb083f1dbd98c6b1b5a",
      "2327fd12b01c4f85bdfa0a8c4120acb5",
      "d3f23be7808d44d1816a7c08300cbc92",
      "3f9a628c822f4f17b342d99dd5992172",
      "9a3f0de3141241f3beeee0a1eaa99751",
      "ac87fe8ca6d149ba9bf1a04bdbddd33a",
      "cb12d0fa832b40c68a2578e15141d804",
      "8cf77ed33e59441fbb8954ea01ab1671",
      "2c339474bb164b25a4195048ee0d1a29"
     ]
    },
    "outputId": "21951204-5057-4fa4-9029-8962d4f70d38"
   },
   "outputs": [],
   "source": [
    "#@markdown The plots below show the distance to the optimum for both variables\n",
    "#@markdown accros the two methos, as well as the parameter trajectory over the loss\n",
    "#@markdown surface.\n",
    "\n",
    "# Compute loss surface\n",
    "g = lambda u, v: loss_2d(copy.deepcopy(model), u, v)\n",
    "Z = np.fromiter(map(g, U.ravel(), V.ravel()), U.dtype).reshape(V.shape)\n",
    "min_idx = np.unravel_index(np.argmin(Z), Z.shape)\n",
    "best_u, best_v = U[min_idx], V[min_idx]\n",
    "\n",
    "# Initialization of the variables\n",
    "INITS = [2.5, 3.7]\n",
    "\n",
    "# Used for plotting\n",
    "LABELS = ['GD', 'Momentum']\n",
    "COLORS = ['black', 'red']\n",
    "LSTYLES = ['-', '--']\n",
    "\n",
    "@widgets.interact\n",
    "def momentum_experiment(max_steps=widgets.IntSlider(200, 50, 500, 5),\n",
    "                        lr=widgets.FloatLogSlider(value=1e-1, min=-3, max=0, step=0.1)):\n",
    "\n",
    "    # Execute both optimizers\n",
    "    sgd_traj = run_optimizer(INITS, eval_fn=g, update_fn=gradient_update,\n",
    "                    max_steps=max_steps, optim_kwargs={'lr': lr})\n",
    "    mom_traj = run_optimizer(INITS, eval_fn=g, update_fn=momentum_update,\n",
    "                    max_steps=max_steps, optim_kwargs={'lr': lr, 'beta':0.9})\n",
    "\n",
    "    TRAJS = [sgd_traj, mom_traj]\n",
    "\n",
    "    # Plot distances\n",
    "    fig = plt.figure(figsize=(9,4))\n",
    "    plot_param_distance(best_u, best_v, TRAJS, fig, LSTYLES, LABELS, use_log=True)\n",
    "\n",
    "    # # Plot trajectories\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    with plt.xkcd():\n",
    "        ax = plot_surface(U, V, Z, fig)\n",
    "        for traj, c, label in zip(TRAJS, COLORS, LABELS):\n",
    "            ax.plot3D(*traj.T, c, linewidth=0.3, label=label)\n",
    "            ax.scatter3D(*traj.T, '.-', s=1, c=c);\n",
    "\n",
    "        # Plot optimum point\n",
    "        ax.scatter(best_u, best_v, Z.min(), marker='*', s=80, c='lime', label='Opt.');\n",
    "\n",
    "        from matplotlib.lines import Line2D\n",
    "        lines = [Line2D([0], [0], color=c, linewidth=3, linestyle='--') for c in COLORS]\n",
    "        lines.append(Line2D([0], [0], color='lime', linewidth=0, marker='*'))\n",
    "        ax.legend(lines, LABELS + ['Optimum'], loc='right', bbox_to_anchor=(.8, -0.1), ncol=len(LABELS)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks:**  Note that the loss has very different curvatures between the weight and the bias dimensions: a change of the same magnitude has a much bigger effect in the loss when applied to the weight than to the bias. This leads to very slow convergence of GD in the weight dimension.\n",
    "\n",
    "Momentum encourages 'movement' in previously seen directions and results in  both parameters converging much faster. The oscillations you can see for the momentum trajectories arise due to overshooting past the solution due to the momentum term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Non-convexity\n",
    "\n",
    "The introduction of even just 1 hidden layer in the neural network transforms the previous convex optimization problem into a non-convex one. And with great non-convexity, comes great responsibility... (Sorry, we couldn't help it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "4f25bb0f-4375-43f4-9d5a-8b8d4587ed47"
   },
   "outputs": [],
   "source": [
    "#@title Video 5: Overparametrization\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"7vUpUEKKl5o\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a couple of minutes to play with a more complex 3D visualization of the loss landscape of a neural network on a non-convex problem. Visit https://losslandscape.com/explorer.\n",
    "\n",
    "1. Explore the features on the bottom left corner. You can see an explanation for each icon by clicking on the [i] button located on the top right corner.\n",
    "2. Use the 'gradient descent' feature to perform a thought experiment:\n",
    "    -   Choose an initialization\n",
    "    -   Choose the learning rate\n",
    "    -   Mentally formulate your hypothesis about what kind of trajectory you expect to observe \n",
    "3. Run the experiment and contrast your intuition with the observed behavior.\n",
    "4. Repeat this experiment a handful of times for several initialization/learning rate configurations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overparametrization to the rescue!\n",
    "\n",
    "As you may have seen, the non-convex nature of the surface can lead the optimization process to get stuck in undesirable local-optima. There is ample empirical evidence supporting the claim that 'overparameterized' models are easier to train.\n",
    "\n",
    "We will explore this assertion in the context of our MLP training. For this, we initialize a fixed model and construct several models by small random perturbations to the original initialized weights. Now, we train each of these perturbed models and see how the loss evolves. If we were in the convex setting, we should reach very similar objective values upon convergence since all these models were very close at the beginning of training, and in convex problems, every local optimum is also a global optimum.\n",
    "\n",
    "Use the interactive plot below to visualize the loss progression for these perturbed models:\n",
    "\n",
    "1. Select different settings from the `hidden_dims` drop-down menu.\n",
    "2. Explore the effect of the number of steps and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449,
     "referenced_widgets": [
      "8db6e6be5a094effbd6b6213b2439a00",
      "7aea49314dea47c8bac1aedf2f80b778",
      "1e8d58da79de4c2fa9bf36c4cdc535e5",
      "e6e5a66d9afb4c71aceebd23e0f981c4",
      "7ed5bbc5a176466080d4f98c4d29d9bb",
      "82565571156848fa82bc92655009ef7b",
      "0a56200c345949a79877a6f9abdc45ee",
      "f08b4f1a9cba47ff91d11307e6a3b4fd",
      "914ea88a29fc4787b17f4a5cd410a8bb",
      "30a6a039bda64955997edced94cef763",
      "5d161e1d32e54d19b154e208b2c7f9b2",
      "c193bee51c184d3ebc9793adcfe3a86a",
      "8cbb482bff624e8690ed48e5ba3e4790",
      "7414d6e957e945f390f745ef3b6ff182",
      "97990ed0367f42019bd600bd2e164e76",
      "2734dfb3f3674fc79bd9fabe7cea837d"
     ]
    },
    "outputId": "0e8d448f-3fcf-4f1e-fb76-7aef10475258"
   },
   "outputs": [],
   "source": [
    "#@markdown Execute this cell to enable the widget!\n",
    "\n",
    "@widgets.interact\n",
    "def overparam(max_steps=widgets.IntSlider(150, 50, 500, 5),\n",
    "              hidden_dims=widgets.Dropdown(options=[\"10\", \"20, 20\", \"100, 100\"],\n",
    "                                           value=\"10\"),\n",
    "              lr=widgets.FloatLogSlider(value=5e-2, min=-3, max=0, step=0.1),\n",
    "              num_inits=widgets.IntSlider(7, 5, 10, 1)):\n",
    "\n",
    "    print('Generating plot')\n",
    "\n",
    "    X, y = train_set.data[subset_index, :], train_set.targets[subset_index]\n",
    "\n",
    "    hdims = [int(s) for s in hidden_dims.split(',')]\n",
    "    base_model = MLP(in_dim=784, out_dim=10, hidden_dims=hdims)\n",
    "\n",
    "    with plt.xkcd():\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(5,4))\n",
    "\n",
    "        for _ in range(num_inits):\n",
    "\n",
    "            model = copy.deepcopy(base_model)\n",
    "            random_update(model, noise_scale=2e-1)\n",
    "\n",
    "            loss_hist = np.zeros((max_steps, 2))\n",
    "            for step in range(max_steps):\n",
    "                loss = loss_fn(model(X), y)\n",
    "                gradient_update(loss, list(model.parameters()), lr=lr)\n",
    "                loss_hist[step] = np.array([step, loss.item()])\n",
    "\n",
    "            plt.plot(loss_hist[:, 0], loss_hist[:, 1])\n",
    "\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0, 3);\n",
    "\n",
    "    num_params = sum([np.prod(_.shape) for _ in model.parameters()])\n",
    "    print('Plot ready - Model # parameters:  ' + str(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks:** 'Overparameterized' models reliably lead to lower loss values regardless of the perturbation of the parameters at initialization, reaching very similar local optima despite the non-convexity of the landscape. This is in stark constrast for models with low number of paramers, for which the model can get stuck in local optima of a relatively big variance in terms of the objective function values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Full gradients are expensive\n",
    "\n",
    "So far we have used only a small (fixed) subset of 500 trainig examples to performe the updates on the model parameters in our quest to minimize the loss. But what if we decided to use the training set? Do our current approach scale to datasets with tens of thousands, or millions of datapoints?\n",
    "\n",
    "In this section we explore an efficient alternative to avoid having to perform computations on all the training examples before performing a parameter update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "a61b3613-af50-4296-cc19-11540b9314cb"
   },
   "outputs": [],
   "source": [
    "#@title Video 6: Mini-batches\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"hbqUxpNBUGk\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost of computation\n",
    "\n",
    "Evaluating a neural network is a relatively fast process. However, when repeated millions of times, the computational cost of performing forward and backwards passes through the network starts becoming significant.\n",
    "\n",
    "In the visualization below, we show the time (averaged over 10 runs) of computing a forward and backward pass with a changing number of input examples. Choose from the different options in the drop-down box and note how the vertical scale changes depending on the size of the network. \n",
    "\n",
    "**Remarks:** Note the computational cost of a forward pass follows a clear linear trend depending on the number of input examples, as well as how the cost of the corresponding backward pass has a similar computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319,
     "referenced_widgets": [
      "334cbd6e251d4a2b8b340e4664e6e3dc",
      "695a4209315d4e7eb2d16c088f328cae",
      "a2d0ac8f6bb648008568b956968eb9db",
      "ae7064c91ff7486a9cc629288d626f3f",
      "76a0cde740c84c0fa29aca68f74f0f7b",
      "d9cb5821a4464614b946171ce0762123",
      "157d65665d104426bae01eb8746274cf"
     ]
    },
    "outputId": "f733201b-e38c-4035-f07d-6793df0a8d64"
   },
   "outputs": [],
   "source": [
    "#@markdown Execute this cell to enable the widget!\n",
    "\n",
    "def measure_update_time(model, num_points):\n",
    "    X, y = train_set.data[:num_points], train_set.targets[:num_points]\n",
    "    start_time = time.time()\n",
    "\n",
    "    loss = loss_fn(model(X), y)\n",
    "    loss_time = time.time()\n",
    "\n",
    "    gradient_update(loss, list(model.parameters()), lr=0)\n",
    "    gradient_time = time.time()\n",
    "\n",
    "    return loss_time - start_time, gradient_time - loss_time\n",
    "\n",
    "@widgets.interact\n",
    "def computation_time(hidden_dims=widgets.Dropdown(options=[\"10\",\n",
    "                                                           \"100\",\n",
    "                                                           \"50, 50\"],\n",
    "                                           value=\"10\")):\n",
    "\n",
    "    hdims = [int(s) for s in hidden_dims.split(',')]\n",
    "    model = MLP(in_dim=784, out_dim=10, hidden_dims=hdims)\n",
    "\n",
    "    NUM_POINTS = [1, 5, 10, 100, 200, 500, 1000, 5000, 10000, 20000, 30000, 50000]\n",
    "    times_list = []\n",
    "    for _ in range(5):\n",
    "        times_list.append(np.array([measure_update_time(model, _) for _ in NUM_POINTS]))\n",
    "\n",
    "    times = np.array(times_list).mean(axis=0)\n",
    "\n",
    "\n",
    "    with plt.xkcd():\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(5,4))\n",
    "        plt.plot(NUM_POINTS, times[:, 0], label='Forward')\n",
    "        plt.plot(NUM_POINTS, times[:, 1], label='Backward')\n",
    "        plt.xlabel('Number of data points')\n",
    "        plt.ylabel('Seconds')\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement minibatch sampling\n",
    "\n",
    "Complete the code in `sample_minibatch` so as to produce IID subsets of the training set of the desired size. (This is _not_ a trick question.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_minibatch(input_data, target_data, num_points=100):\n",
    "    \"\"\"Sample a minibatch of size num_point from the provided input-target data\n",
    "\n",
    "    Args:\n",
    "        input_data (tensor): Multi-dimensional tensor containing the input data\n",
    "        input_data (tensor): 1D tensor containing the class labels\n",
    "        num_points (int): Number of elements to be included in minibatch\n",
    "\n",
    "    Returns:\n",
    "        batch_inputs (tensor): Minibatch inputs\n",
    "        batch_targets (tensor): Minibatch targets\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #################################################\n",
    "    ## TODO for students: sample minibatch of data ##\n",
    "\n",
    "    # Sample a collection of IID indices from the existing data\n",
    "    batch_indices = ...\n",
    "\n",
    "    # Use batch_indices to extract entries from the input and target data tensors\n",
    "    batch_inputs = input_data[...]\n",
    "    batch_targets = target_data[...]\n",
    "\n",
    "    raise NotImplementedError(\"Student exercise: implement gradient update\")\n",
    "    #################################################\n",
    "\n",
    "    return batch_inputs, batch_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D4_Optimization/solutions/W1D4_tutorial_Solution_c3a7e4d3.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different minibatch sizes\n",
    "\n",
    "What are the trade-offs induced by the choice of minibatch size? The interactive plot below show the training evolution of a 2-hidden layer MLP with 100 hidden units in each hidden layer. Different plots correspond to a different choice of minibatch size. We have a fixed time budget for all the cases, reflected in the horizontal axes of these plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310,
     "referenced_widgets": [
      "3326c0fe50b040aa889aa227db61d1a2",
      "724867a582f0428ebcda98b81bf04bfd",
      "d031a0aaf34e4395a80c60c146ee0510",
      "eb1bd031529f44d2adb5d6c93b7460d9",
      "38390bc14e4e469eae6d1f89674ceac0",
      "720cc51d3c2144db8bac0d282e6a532e",
      "db3d390c80494882aa7458d58980039b",
      "1c23dc89260a4e5f98a8b89a6891b859",
      "c1ba7b24e6f74ba998e46edc32e6d066",
      "a75507b677e346d680f1ba763182fe25",
      "bdf6d1f4c0eb4651b6cb1700df1eb8a6",
      "ac63045dd7c4432cb8a458253cbdfbfc",
      "58b34276f2fb4bfe98d3ea1389215a20"
     ]
    },
    "outputId": "13fa84d6-9fe2-4a7b-d0ec-4f93a245b967"
   },
   "outputs": [],
   "source": [
    "#@markdown Execute this cell to enable the widget!\n",
    "\n",
    "@widgets.interact\n",
    "def minibatch_experiment(batch_sizes='20, 250, 1000',\n",
    "                         lrs='5e-3, 5e-3, 5e-3',\n",
    "                         time_budget=widgets.FloatSlider(value=5.0, min=3.0, max=10., step=0.1)):\n",
    "\n",
    "    batch_sizes = [int(s) for s in batch_sizes.split(',')]\n",
    "    lrs = [float(s) for s in lrs.split(',')]\n",
    "\n",
    "    LOSS_HIST = {_:[] for _ in batch_sizes}\n",
    "\n",
    "    X, y = train_set.data, train_set.targets\n",
    "    base_model = MLP(in_dim=784, out_dim=10, hidden_dims=[100, 100])\n",
    "\n",
    "    for id, batch_size in enumerate(batch_sizes):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Create a new copy of the model for each batch size\n",
    "        model = copy.deepcopy(base_model)\n",
    "        params = list(model.parameters())\n",
    "        lr = lrs[id]\n",
    "\n",
    "        # Fixed budget per choice of batch size\n",
    "        while (time.time() - start_time) < time_budget:\n",
    "            batch_x, batch_y = sample_minibatch(X, y, batch_size)\n",
    "            loss = loss_fn(model(batch_x), batch_y)\n",
    "            gradient_update(loss, params, lr=lr)\n",
    "            LOSS_HIST[batch_size].append([time.time() - start_time, loss.item()])\n",
    "\n",
    "    with plt.xkcd():\n",
    "        fig, axs = plt.subplots(1, len(batch_sizes), figsize=(10,3))\n",
    "\n",
    "        for ax, batch_size in zip(axs, batch_sizes):\n",
    "\n",
    "            plot_data = np.array(LOSS_HIST[batch_size])\n",
    "            ax.plot(plot_data[:, 0], plot_data[:, 1], label=batch_size, alpha=0.8)\n",
    "            ax.set_title('Batch size: ' + str(batch_size))\n",
    "            ax.set_xlabel('Seconds')\n",
    "            ax.set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks:** SGD works! We have an algorithm that can be applied (with the due precautions) to learn datasets of arbitrary size. \n",
    "\n",
    "However, note the diference in the vertical scale across the plots above. When using a larger minibatch, we can perform fewer parameter updates as the forward and backward passes are more expensive.\n",
    "\n",
    "This highlights the interplay between the minibatch size and the learning rate: when our minibatch is larger, we have a more confident estimator of the direction to move, and thus can afford a larger learning rate. On the other hand, extremely small minibatches are very fast computationally but are not representative of the data distribution and yield estimations of the gradient with high variance.\n",
    "\n",
    "We encourage you to tune the value of the learning rate for each of the minibatch sizes in the previous demo, to achieve a training loss steadily below 0.5 within 5 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Hyperparameter tuning - Adaptive methods\n",
    "\n",
    "As of now, you should be aware that there are many knobs to turn when working on a machine learning problem. Some of these relate to the optimization algorithm, to the choice of model, or to the objective to minimize. Here are some prototypical examples:\n",
    "\n",
    "- Problem: loss function, regularization coefficients (Day 5)\n",
    "- Model: architecture, activations function\n",
    "- Optimizer: learning rate, batch size, momentum coefficient\n",
    "\n",
    "We concentrate on the choices that are directly related with optimization. In particular, we will explore some _automatic_ methods for setting the learning rate, in a way that fixes the poor-conditioning problem and is robust across different problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "29be438d-89dc-4d08-8043-47926aa2617d"
   },
   "outputs": [],
   "source": [
    "#@title Video 7: Adaptive Methods\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"rxJfoTw20x8\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Implement RMSprop\n",
    "\n",
    "In this exercise you will implement the update of the RMSprop optimizer:\n",
    "\n",
    "$$ w_{t+1} = w_t - \\eta \\frac{\\nabla J(w_t)}{\\sqrt{v_t + \\epsilon}}$$\n",
    "\n",
    "$$ v_{t+1} = \\alpha v_t + (1 - \\alpha) \\nabla J(w_t)^2,$$\n",
    "\n",
    "where the non-standard operations (division of two vectors, squaring a vector, etc) are to be interpreted as element-wise operations, i.e., the operation is applied to each (pair of) entry[ies] of the vector(s) considered as real number(s).\n",
    "\n",
    "Here, the $\\epsilon$ hyperparameter provides numerical estability to the algorithm, by preventing the learning rate to become too big when $v_t$ is small. Typically, we set $\\epsilon$ to a default small value, like $10^{-8}$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop_update(loss, params, grad_sq, lr=1e-1, alpha=0.8):\n",
    "    \"\"\"Perform an RMSprop update on a collection of parameters\n",
    "\n",
    "    Args:\n",
    "        loss (tensor): A scalar tensor containing the loss whose gradient will be computed\n",
    "        params (iterable): Collection of parameters with respect to which we compute gradients\n",
    "        grad_sq (iterable): Moving average of squared gradients\n",
    "        lr (float): Scalar specifying the learning rate or step-size for the update\n",
    "        alpha (float): Moving average parameter\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Clear up gradients as Pytorch automatically accumulates gradients from\n",
    "    # successive backward calls\n",
    "    zero_grad(params)\n",
    "\n",
    "    # Compute gradients on given objective\n",
    "    loss.backward()\n",
    "\n",
    "    for (par, gsq) in zip(params, grad_sq):\n",
    "\n",
    "        #################################################\n",
    "        ## TODO for students: update the value of the parameter ##\n",
    "\n",
    "        # Update estimate of gradient variance\n",
    "        gsq.data = alpha * gsq.data + (1-alpha) * par.grad**2\n",
    "\n",
    "        # Update parameters\n",
    "        par.data -=  lr * (par.grad / (1e-8 + gsq.data)**0.5)\n",
    "\n",
    "        raise NotImplementedError(\"Student exercise: implement gradient update\")\n",
    "        #################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D4_Optimization/solutions/W1D4_tutorial_Solution_28ebe622.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare optimizers\n",
    "\n",
    "Below, we compare your implementations of SGD, momentum and RMSprop. If you have successfully coded all the exercises so far: congrats! You are *now in the know* of some of the most commonly used and powerful tools of optimization for deep learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310,
     "referenced_widgets": [
      "efb49cb2ae554403ada91003f310b0b0",
      "266f5d67e59a4786b7cfe8e2ec894c02",
      "203a05a2261546a5a330370490d7c453",
      "f7637e6627de46fcb269df224765b78b",
      "a1b661cf515247deb587a259b798a8a6",
      "85c4cddf05594219923f625d01e41bb4",
      "15ea556cd64347e086ed604c5c64f7ca",
      "de33d5854c9749f5bd889a6140e06314",
      "0c201c60fc084df9a679d08c13af3670",
      "fa01b043742e46ff8fde7c5dda6f676c",
      "cd173cf16c7c4dc89a30fee8ee5cf720",
      "de42d521c56a47fe8f5e0de6cf4f0521",
      "8f811c83b31e4d4e8f68a789c24d6dd5"
     ]
    },
    "outputId": "be6d956a-7fa6-400e-d7d7-a58007beed50"
   },
   "outputs": [],
   "source": [
    "#@markdown Execute this cell to enable the widget!\n",
    "\n",
    "X, y = train_set.data, train_set.targets\n",
    "\n",
    "@widgets.interact\n",
    "def compare_optimizers(batch_size=(25, 250, 5),\n",
    "                       lr=widgets.FloatLogSlider(value=2e-3, min=-5, max=0),\n",
    "                       max_steps=(50, 500, 5)):\n",
    "\n",
    "    SGD_DICT = [gradient_update, 'SGD', 'black', '-', {'lr': lr}]\n",
    "    MOM_DICT = [momentum_update, 'Momentum', 'red', '--', {'lr': lr, 'beta': 0.8}]\n",
    "    RMS_DICT = [rmsprop_update, 'RMSprop', 'fuchsia', '-', {'lr': lr, 'alpha': 0.8}]\n",
    "\n",
    "    ALL_DICTS = [SGD_DICT, MOM_DICT, RMS_DICT]\n",
    "\n",
    "    base_model = MLP(in_dim=784, out_dim=10, hidden_dims=[100, 100])\n",
    "\n",
    "    LOSS_HIST = {}\n",
    "\n",
    "    for opt_dict in ALL_DICTS:\n",
    "        update_fn, opt_name, color, lstyle, kwargs = opt_dict\n",
    "        LOSS_HIST[opt_name] = []\n",
    "\n",
    "        model = copy.deepcopy(base_model)\n",
    "        params = list(model.parameters())\n",
    "\n",
    "        if opt_name != 'SGD':\n",
    "            aux_tensors = [torch.zeros_like(_) for _ in params]\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            batch_x, batch_y = sample_minibatch(X, y, batch_size)\n",
    "            loss = loss_fn(model(batch_x), batch_y)\n",
    "\n",
    "            if opt_name == 'SGD':\n",
    "                update_fn(loss, params, **kwargs)\n",
    "            else:\n",
    "                update_fn(loss, params, aux_tensors, **kwargs)\n",
    "\n",
    "            LOSS_HIST[opt_name].append(loss.item())\n",
    "\n",
    "    with plt.xkcd():\n",
    "        fig, axs = plt.subplots(1, len(ALL_DICTS), figsize=(9,3))\n",
    "\n",
    "        for ax, optim_dict in zip(axs, ALL_DICTS):\n",
    "\n",
    "            opt_name = optim_dict[1]\n",
    "            ax.plot(range(max_steps), LOSS_HIST[opt_name], alpha=0.8)\n",
    "            ax.set_title(opt_name)\n",
    "            ax.set_xlabel('Iteration')\n",
    "            ax.set_ylabel('Loss')\n",
    "            ax.set_ylim(0, 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Remarks:** Note that RMSprop is allowing us to use a 'per-dimension' learning rate _without having to tune one learning rate for each dimension **ourselves**_. The method uses information collected about the variance of the gradients throughout training to **adapt** the step size for each of the parameters automatically. The savings in tuning efforts of RMSprop over SGD or 'plain' momentum are undisputed on this task. \n",
    "\n",
    "Moreover, adaptive optimization methods are currently a highly active research domain, with many related algorithms like Adam, AMSgrad, Adagrad being used in practical application and theoretically investigated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Putting it all together\n",
    "\n",
    "We have progressively built a sophisticated optimization algorithm which is able to deal with a non-convex, poor-conditioned problem concerning tens of thousands of training examples. Now we present _you_ with a small challenge: beat us! :P\n",
    "\n",
    "Your mission is to train an MLP model with 3 hidden layers of [200, 100, 50] neurons that can compete with a benchmark model (of the same architecture) which we provide. In this section you will be able to use the full Pytorch power: loading the data, defining the model, sampling minibatches as well as Pytorch's **optimizer implementations**. \n",
    "\n",
    "There is a big engineering component behind the design of optimizers and their implementation can sometimes become tricky. So unless you are directly doing research in optimization, it's recommended to use an implementation provided by a widely reviewed open-source library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "969248ad-d056-47f0-b01f-4a964e37ad8f"
   },
   "outputs": [],
   "source": [
    "#@title Video 8: Putting it all together\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"DP9c13vLiOM\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1716d99b-8836-44b1-c572-c540c2389006"
   },
   "outputs": [],
   "source": [
    "# Download parameters of the benchmark model\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://jgalle29.github.io/resources/benchmark_model.pt\", \"benchmark_model.pt\")\n",
    "\n",
    "# Create MLP object and update weights with those of saved model\n",
    "benchmark_model = MLP(in_dim=784, out_dim=10, hidden_dims=[200, 100, 50])\n",
    "benchmark_model.load_state_dict(torch.load('benchmark_model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an optimizer in the following steps:\n",
    "\n",
    "1. Load  the corresponding class that implements the parameter updates and other internal management activities, including:\n",
    "    - create auxiliary variables,\n",
    "    - update moving averages,\n",
    "    - adjust learning rate.\n",
    "2. Pass the parameters of the Pytorch model that the optimizer has control over. Note that different parameter groups can potentially be controlled by different optimizers.\n",
    "3. Specify hyperparameters, including learning rate, momentum, moving average factors, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Train your own model\n",
    "\n",
    "Now, train the model with your preferred optimizer and find a good combination of hyperparameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note to reviewers: should we use cuda in this section? Has this been covered in\n",
    "# past days?\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "## TODO for students: adjust hyper-parameters ##\n",
    "\n",
    "MAX_EPOCHS = 1\n",
    "LR = 1e-4\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Define the model and associated optimizer -- to keep things fair, we use the\n",
    "# same architecture.\n",
    "model = MLP(in_dim=784, out_dim=10, hidden_dims=[200, 100, 50])\n",
    "\n",
    "if USE_CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "# You might want to try out different optimizers\n",
    "# Options: torch.optim.RMSprop, torch.optim.Adagrad, torch.optim.Adam\n",
    "# Check the optimizer documentation and hyperparameter meaning before using!\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "836337a9-bc03-4a6a-f27f-764a3c2a27f3"
   },
   "outputs": [],
   "source": [
    "# Load data using a Pytorch Dataset\n",
    "train_set, test_set = load_mnist_data(change_tensors=False)\n",
    "\n",
    "# Create the corresponding DataLoaders for training and test\n",
    "train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=256)\n",
    "\n",
    "# Run training\n",
    "metrics = {'loss':[], 'acc':[]}\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "\n",
    "    running_loss, running_acc = 0., 0.\n",
    "\n",
    "    for batch_id, batch in enumerate(train_loader):\n",
    "\n",
    "        # Extract minibatch data\n",
    "        batch_x, batch_y = batch[0].to(DEVICE), batch[1].to(DEVICE)\n",
    "\n",
    "        # Just like before, refresh gradient accumulators.\n",
    "        # Note that this is now a method of the optimizer.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Evaluate model and loss on minibatch\n",
    "        preds = model(batch_x)\n",
    "        loss = loss_fn(preds, batch_y)\n",
    "        acc = torch.mean(1.0 * (preds.argmax(dim=1) == batch_y))\n",
    "\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        # Note how all the magic in the update of the parameters is encapsulated by\n",
    "        # the optimizer class.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log metrics for plotting\n",
    "        metrics['loss'].append(loss.cpu().item())\n",
    "        metrics['acc'].append(acc.cpu().item())\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.cpu().item()\n",
    "        running_acc += acc.cpu().item()\n",
    "        if batch_id % 100 == 99: # Log every 100 batches\n",
    "            print('Epoch %d - Batch %d - Loss: %.3f - Acc: %.3f' %\n",
    "                  (epoch + 1, batch_id + 1, running_loss / 100, running_acc / 100))\n",
    "            running_loss, running_acc = 0., 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "outputId": "607ef357-f7bc-4b8f-a828-8885e7a93caf"
   },
   "outputs": [],
   "source": [
    "with plt.xkcd():\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
    "\n",
    "    ax[0].plot(range(len(metrics['loss'])), metrics['loss'])\n",
    "    ax[0].set_xlabel('Iteration')\n",
    "    ax[0].set_ylabel('Train Loss')\n",
    "\n",
    "    ax[1].plot(range(len(metrics['acc'])), metrics['acc'])\n",
    "    ax[1].set_xlabel('Iteration')\n",
    "    ax[1].set_ylabel('Train Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8b7c7784-81dc-4cf0-df92-bfd134c36a9b"
   },
   "outputs": [],
   "source": [
    "# Performance of your model\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "\n",
    "    loss_log, acc_log = [], []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch_id, batch in enumerate(data_loader):\n",
    "\n",
    "            # Extract minibatch data\n",
    "            batch_x, batch_y = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "            # Evaluate model and loss on minibatch\n",
    "            preds = model(batch_x)\n",
    "            loss_log.append(loss_fn(preds, batch_y).item())\n",
    "            acc_log.append(torch.mean(1. * (preds.argmax(dim=1) == batch_y)).item())\n",
    "\n",
    "    return np.mean(loss_log), np.mean(acc_log)\n",
    "\n",
    "print('Your model')\n",
    "train_loss, train_accuracy = eval_model(model, train_loader)\n",
    "test_loss, test_accuracy = eval_model(model, test_loader)\n",
    "print('Train/Test Loss', train_loss, test_loss)\n",
    "print('Train/Test Accuracy', train_accuracy, test_accuracy)\n",
    "\n",
    "print('\\nBenchmark model')\n",
    "train_loss, train_accuracy = eval_model(benchmark_model, train_loader)\n",
    "test_loss, test_accuracy = eval_model(benchmark_model, test_loader)\n",
    "print('Train/Test Loss', train_loss, test_loss)\n",
    "print('Train/Test Accuracy', train_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Ethical concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "379cefee-f85f-4972-ff9f-e6b8a7a3e0d3"
   },
   "outputs": [],
   "source": [
    "#@title Video 9: Ethical concerns\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"0EthSI0cknI\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "* Optimization is necessary to create Deep Learning models that are guaranteed to converge \n",
    "* Stochastic Gradient Descent and Momentum are two commonly used optimization techniques\n",
    "* RMSProp is a way of adaptive hyper parameter tuning which utilises a per-dimension learning rate\n",
    "* Poor choice of optimization functions can lead to unforeseen undesirable consequences \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D4_tutorial",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
